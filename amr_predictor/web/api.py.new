"""
Web API for AMR Predictor.

This module provides FastAPI endpoints for the AMR Predictor functionality,
enabling integration with web applications.
"""

import os
import tempfile
import json
import uuid
from typing import List, Dict, Optional, Any, Union
from datetime import datetime
from fastapi import FastAPI, File, UploadFile, HTTPException, BackgroundTasks, Query, Path, Body, Request, Form
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel, Field

# Try to import CORS middleware
try:
    from fastapi.middleware.cors import CORSMiddleware
    CORS_AVAILABLE = True
except ImportError:
    CORS_AVAILABLE = False

from ..core.utils import logger, ProgressTracker, ensure_directory_exists, get_default_output_path
from ..core.prediction import PredictionPipeline
from ..processing.aggregation import PredictionAggregator
from ..processing.sequence_processing import SequenceProcessor
from ..processing.visualization import VisualizationGenerator

# Create FastAPI app
app = FastAPI(
    title="AMR Predictor API",
    description="API for predicting antimicrobial resistance from genomic sequences",
    version="1.0.0"
)

# Add custom exception handler for validation errors
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    # Handle the errors without trying to include the entire body
    return JSONResponse(
        status_code=422,
        content={
            "detail": str(exc.errors()),
            "message": "Validation error - check your request format"
        }
    )

# Add CORS middleware if available
if CORS_AVAILABLE:
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # Update with specific origins in production
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

# Define directories for uploads and results
UPLOAD_DIR = os.path.join(os.getcwd(), "uploads")
RESULTS_DIR = os.path.join(os.getcwd(), "results")

# Ensure directories exist
ensure_directory_exists(UPLOAD_DIR)
ensure_directory_exists(RESULTS_DIR)

# Store job status in memory (in production, use a proper database)
jobs = {}

# Pydantic models for API
class PredictionRequest(BaseModel):
    """Request model for prediction endpoint"""
    model_name: str = Field(default="alakob/DraGNOME-2.5b-v1", description="HuggingFace model name or path")
    batch_size: int = Field(default=8, description="Batch size for predictions", ge=1)
    segment_length: int = Field(default=6000, description="Maximum segment length, 0 to disable splitting", ge=0)
    segment_overlap: int = Field(default=0, description="Overlap between segments", ge=0)
    use_cpu: bool = Field(default=False, description="Force CPU inference instead of GPU")
    resistance_threshold: float = Field(default=0.5, description="Threshold for resistance classification", ge=0.0, le=1.0)
    enable_sequence_aggregation: bool = Field(default=True, description="Enable sequence-level aggregation of results")

class AggregationRequest(BaseModel):
    """Request model for aggregation endpoint"""
    model_suffix: str = Field(default="_all_107_sequences_prediction", description="Suffix to remove from filenames when extracting model names")
    file_pattern: str = Field(default="*_all_107_sequences_prediction.txt", description="File pattern to match for input files")

class SequenceProcessingRequest(BaseModel):
    """Request model for sequence processing endpoint"""
    resistance_threshold: float = Field(default=0.5, description="Threshold for resistance classification", ge=0.0, le=1.0)

class VisualizationRequest(BaseModel):
    """Request model for visualization endpoint"""
    step_size: int = Field(default=1200, description="Step size in base pairs for WIG format", ge=1)

class JobResponse(BaseModel):
    """Response model for job status"""
    job_id: str
    status: str
    progress: float = 0.0
    start_time: str
    end_time: Optional[str] = None
    result_file: Optional[str] = None
    aggregated_result_file: Optional[str] = None
    error: Optional[str] = None
    additional_info: Optional[Dict[str, Any]] = None

# Web-specific progress tracker
class WebProgressTracker(ProgressTracker):
    """
    Web-specific progress tracker that updates job status.
    """
    
    def __init__(self, job_id: str, total_steps: int = 100):
        """
        Initialize the web progress tracker.
        
        Args:
            job_id: The job ID to update
            total_steps: Total number of steps in the operation
        """
        super().__init__(total_steps=total_steps, callback=self._update_job_status)
        self.job_id = job_id
    
    def _update_job_status(self, tracker):
        # Update the job status in the jobs dictionary
        if self.job_id in jobs:
            jobs[self.job_id]["progress"] = tracker.percentage
            jobs[self.job_id]["status"] = tracker.status
            
            if tracker.error:
                jobs[self.job_id]["error"] = tracker.error
                jobs[self.job_id]["status"] = "Error"
                
            if tracker.additional_info:
                if "additional_info" not in jobs[self.job_id]:
                    jobs[self.job_id]["additional_info"] = {}
                jobs[self.job_id]["additional_info"].update(tracker.additional_info)


# Background task functions
async def predict_task(job_id: str, fasta_path: str, model_name: str, batch_size: int,
                     segment_length: int, segment_overlap: int, use_cpu: bool,
                     resistance_threshold: float, enable_sequence_aggregation: bool):
    """
    Background task for running AMR prediction.
    
    Args:
        job_id: Job ID for tracking
        fasta_path: Path to the FASTA file
        model_name: HuggingFace model name or path
        batch_size: Batch size for predictions
        segment_length: Maximum segment length, 0 to disable splitting
        segment_overlap: Overlap between segments
        use_cpu: Whether to force CPU inference instead of GPU
        resistance_threshold: Threshold for resistance classification (default: 0.5)
        enable_sequence_aggregation: Whether to enable sequence-level aggregation of results
    """
    try:
        # Create output file path
        output_file = os.path.join(RESULTS_DIR, f"amr_predictions_{job_id}.tsv")
        
        # Initialize progress tracker
        progress_tracker = WebProgressTracker(job_id=job_id)
        
        # Initialize pipeline
        pipeline = PredictionPipeline(
            model_name=model_name,
            batch_size=batch_size,
            segment_length=segment_length,
            segment_overlap=segment_overlap,
            device="cpu" if use_cpu else None,
            progress_tracker=progress_tracker,
            enable_sequence_aggregation=enable_sequence_aggregation,
            resistance_threshold=resistance_threshold
        )
        
        # Process the FASTA file
        results = pipeline.process_fasta_file(fasta_path, output_file)
        
        # Update job status
        if "error" in results and results["error"]:
            jobs[job_id].update({
                "status": "Error",
                "error": results["error"],
                "end_time": datetime.now().isoformat()
            })
        else:
            # Prepare job update
            job_update = {
                "status": "Completed",
                "progress": 100.0,
                "result_file": output_file,
                "end_time": datetime.now().isoformat()
            }
            
            # Include aggregated file if available
            if enable_sequence_aggregation and "aggregated_output_file" in results:
                job_update["aggregated_result_file"] = results["aggregated_output_file"]
                
                # Update additional info with aggregation stats
                if "additional_info" not in jobs[job_id]:
                    jobs[job_id]["additional_info"] = {}
                
                if "num_aggregated_sequences" in results:
                    jobs[job_id]["additional_info"]["num_aggregated_sequences"] = results["num_aggregated_sequences"]
            
            # Update the job
            jobs[job_id].update(job_update)
    
    except Exception as e:
        logger.error(f"Error in prediction task: {str(e)}")
        jobs[job_id].update({
            "status": "Error",
            "error": str(e),
            "end_time": datetime.now().isoformat()
        })


async def aggregate_task(job_id: str, file_paths: List[str], model_suffix: str):
    """
    Background task for running AMR aggregation.
    
    Args:
        job_id: Job ID for tracking
        file_paths: List of prediction files to process
        model_suffix: Suffix to remove from filenames when extracting model names
    """
    try:
        # Create output file path
        output_file = os.path.join(RESULTS_DIR, f"amr_aggregated_{job_id}.csv")
        
        # Initialize progress tracker
        progress_tracker = WebProgressTracker(job_id=job_id)
        
        # Initialize aggregator
        aggregator = PredictionAggregator(
            model_suffix=model_suffix,
            progress_tracker=progress_tracker
        )
        
        # Process prediction files
        results = aggregator.process_prediction_files(
            input_files=file_paths,
            output_dir=RESULTS_DIR,
            model_suffix=model_suffix
        )
        
        # Update job status
        if results.empty:
            jobs[job_id].update({
                "status": "Error",
                "error": "Aggregation failed or produced no results",
                "end_time": datetime.now().isoformat()
            })
        else:
            jobs[job_id].update({
                "status": "Completed",
                "progress": 100.0,
                "result_file": output_file,
                "end_time": datetime.now().isoformat()
            })
    
    except Exception as e:
        logger.error(f"Error in aggregation task: {str(e)}")
        jobs[job_id].update({
            "status": "Error",
            "error": str(e),
            "end_time": datetime.now().isoformat()
        })


async def process_sequence_task(job_id: str, input_file: str, resistance_threshold: float):
    """
    Background task for running sequence processing.
    
    Args:
        job_id: Job ID for tracking
        input_file: Path to the input prediction file
        resistance_threshold: Threshold for resistance classification
    """
    try:
        # Create output file path
        output_file = os.path.join(RESULTS_DIR, f"sequence_processed_{job_id}.csv")
        
        # Initialize progress tracker
        progress_tracker = WebProgressTracker(job_id=job_id)
        
        # Initialize sequence processor
        sequence_processor = SequenceProcessor(
            resistance_threshold=resistance_threshold,
            progress_tracker=progress_tracker
        )
        
        # Process prediction file
        results = sequence_processor.process_prediction_file(
            input_file=input_file,
            output_file=output_file
        )
        
        # Update job status
        if results.empty:
            jobs[job_id].update({
                "status": "Error",
                "error": "Sequence processing failed or produced no results",
                "end_time": datetime.now().isoformat()
            })
        else:
            jobs[job_id].update({
                "status": "Completed",
                "progress": 100.0,
                "result_file": output_file,
                "end_time": datetime.now().isoformat()
            })
    
    except Exception as e:
        logger.error(f"Error in sequence processing task: {str(e)}")
        jobs[job_id].update({
            "status": "Error",
            "error": str(e),
            "end_time": datetime.now().isoformat()
        })


async def visualize_task(job_id: str, input_file: str, step_size: int):
    """
    Background task for running visualization.
    
    Args:
        job_id: Job ID for tracking
        input_file: Path to the input prediction file
        step_size: Step size in base pairs for WIG format
    """
    try:
        # Create output file paths
        basename = os.path.splitext(os.path.basename(input_file))[0]
        wig_file = os.path.join(RESULTS_DIR, f"{basename}_{job_id}.wig")
        processed_file = os.path.join(RESULTS_DIR, f"{basename}_processed_{job_id}.csv")
        
        # Initialize progress tracker
        progress_tracker = WebProgressTracker(job_id=job_id)
        
        # Initialize visualization generator
        visualizer = VisualizationGenerator(
            progress_tracker=progress_tracker
        )
        
        # Process prediction file
        success = visualizer.generate_wig_from_predictions(
            input_file=input_file,
            output_file=wig_file,
            processed_file=processed_file,
            step_size=step_size
        )
        
        # Update job status
        if not success:
            jobs[job_id].update({
                "status": "Error",
                "error": "Visualization failed or produced no results",
                "end_time": datetime.now().isoformat()
            })
        else:
            jobs[job_id].update({
                "status": "Completed",
                "progress": 100.0,
                "result_file": wig_file,
                "end_time": datetime.now().isoformat()
            })
    
    except Exception as e:
        logger.error(f"Error in visualization task: {str(e)}")
        jobs[job_id].update({
            "status": "Error",
            "error": str(e),
            "end_time": datetime.now().isoformat()
        })


# API endpoints
@app.post("/predict", response_model=JobResponse)
async def predict(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    model_name: str = Form("alakob/DraGNOME-2.5b-v1"),
    batch_size: int = Form(8),
    segment_length: int = Form(6000),
    segment_overlap: int = Form(0),
    use_cpu: bool = Form(False),
    resistance_threshold: float = Form(0.5),
    enable_sequence_aggregation: bool = Form(True)
):
    # Create a params object
    params = PredictionRequest(
        model_name=model_name,
        batch_size=batch_size,
        segment_length=segment_length,
        segment_overlap=segment_overlap,
        use_cpu=use_cpu,
        resistance_threshold=resistance_threshold,
        enable_sequence_aggregation=enable_sequence_aggregation
    )
    logger.debug(f"Using parameters: {params.dict()}")
    """
    Predict antimicrobial resistance from a FASTA file.
    
    Args:
        background_tasks: FastAPI background tasks
        file: FASTA file upload
        params: Prediction parameters
        
    Returns:
        Job response with ID and status
    """
    # Generate job ID
    job_id = str(uuid.uuid4())
    
    # Save uploaded file
    file_path = os.path.join(UPLOAD_DIR, f"{job_id}_{file.filename}")
    
    with open(file_path, "wb") as f:
        contents = await file.read()
        f.write(contents)
    
    # Initialize job status
    jobs[job_id] = {
        "job_id": job_id,
        "status": "Submitted",
        "progress": 0.0,
        "start_time": datetime.now().isoformat(),
        "end_time": None,
        "result_file": None,
        "error": None,
        "additional_info": {
            "input_file": file.filename,
            "model_name": params.model_name,
            "batch_size": params.batch_size,
            "segment_length": params.segment_length,
            "segment_overlap": params.segment_overlap,
            "use_cpu": params.use_cpu,
            "resistance_threshold": params.resistance_threshold,
            "enable_sequence_aggregation": params.enable_sequence_aggregation
        }
    }
    
    # Add task to background tasks
    background_tasks.add_task(
        predict_task,
        job_id=job_id,
        fasta_path=file_path,
        model_name=params.model_name,
        batch_size=params.batch_size,
        segment_length=params.segment_length,
        segment_overlap=params.segment_overlap,
        use_cpu=params.use_cpu,
        resistance_threshold=params.resistance_threshold,
        enable_sequence_aggregation=params.enable_sequence_aggregation
    )
    
    return jobs[job_id]


@app.post("/aggregate", response_model=JobResponse)
async def aggregate(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    model_suffix: str = Form("_all_107_sequences_prediction"),
    file_pattern: str = Form("*_all_107_sequences_prediction.txt")
):
    """
    Aggregate AMR prediction results from multiple files.
    
    Args:
        background_tasks: FastAPI background tasks
        files: List of prediction files to process
        params: Aggregation parameters
        
    Returns:
        Job response with ID and status
    """
    # Generate job ID
    job_id = str(uuid.uuid4())
    
    # Save uploaded files
    file_paths = []
    for file in files:
        file_path = os.path.join(UPLOAD_DIR, f"{job_id}_{file.filename}")
        with open(file_path, "wb") as f:
            contents = await file.read()
            f.write(contents)
        file_paths.append(file_path)
    
    # Initialize job status
    jobs[job_id] = {
        "job_id": job_id,
        "status": "Submitted",
        "progress": 0.0,
        "start_time": datetime.now().isoformat(),
        "end_time": None,
        "result_file": None,
        "error": None,
        "additional_info": {
            "input_files": [file.filename for file in files],
            "model_suffix": model_suffix,
            "file_pattern": file_pattern
        }
    }
    
    # Add task to background tasks
    background_tasks.add_task(
        aggregate_task,
        job_id=job_id,
        file_paths=file_paths,
        model_suffix=model_suffix
    )
    
    return jobs[job_id]


@app.post("/sequence", response_model=JobResponse)
async def process_sequence(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    resistance_threshold: float = Form(0.5)
):
    """
    Process prediction results at the sequence level.
    
    Args:
        background_tasks: FastAPI background tasks
        file: Prediction file to process
        params: Sequence processing parameters
        
    Returns:
        Job response with ID and status
    """
    # Generate job ID
    job_id = str(uuid.uuid4())
    
    # Save uploaded file
    file_path = os.path.join(UPLOAD_DIR, f"{job_id}_{file.filename}")
    
    with open(file_path, "wb") as f:
        contents = await file.read()
        f.write(contents)
    
    # Initialize job status
    jobs[job_id] = {
        "job_id": job_id,
        "status": "Submitted",
        "progress": 0.0,
        "start_time": datetime.now().isoformat(),
        "end_time": None,
        "result_file": None,
        "error": None,
        "additional_info": {
            "input_file": file.filename,
            "resistance_threshold": resistance_threshold
        }
    }
    
    # Add task to background tasks
    background_tasks.add_task(
        process_sequence_task,
        job_id=job_id,
        input_file=file_path,
        resistance_threshold=resistance_threshold
    )
    
    return jobs[job_id]


@app.post("/visualize", response_model=JobResponse)
async def visualize(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    step_size: int = Form(1200)
):
    """
    Convert prediction results to visualization formats.
    
    Args:
        background_tasks: FastAPI background tasks
        file: Prediction file to process
        params: Visualization parameters
        
    Returns:
        Job response with ID and status
    """
    # Generate job ID
    job_id = str(uuid.uuid4())
    
    # Save uploaded file
    file_path = os.path.join(UPLOAD_DIR, f"{job_id}_{file.filename}")
    
    with open(file_path, "wb") as f:
        contents = await file.read()
        f.write(contents)
    
    # Initialize job status
    jobs[job_id] = {
        "job_id": job_id,
        "status": "Submitted",
        "progress": 0.0,
        "start_time": datetime.now().isoformat(),
        "end_time": None,
        "result_file": None,
        "error": None,
        "additional_info": {
            "input_file": file.filename,
            "step_size": step_size
        }
    }
    
    # Add task to background tasks
    background_tasks.add_task(
        visualize_task,
        job_id=job_id,
        input_file=file_path,
        step_size=step_size
    )
    
    return jobs[job_id]


@app.get("/jobs/{job_id}", response_model=JobResponse)
async def get_job_status(job_id: str = Path(..., description="Job ID to check")):
    """
    Get the status of a job.
    
    Args:
        job_id: Job ID to check
        
    Returns:
        Job response with current status
    """
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    return jobs[job_id]


@app.get("/jobs/{job_id}/download")
async def download_result(job_id: str = Path(..., description="Job ID to download results for")):
    """
    Download the result file for a job.
    
    Args:
        job_id: Job ID to download results for
        
    Returns:
        File response with the result file
    """
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    job = jobs[job_id]
    
    if job["status"] != "Completed":
        raise HTTPException(status_code=400, detail=f"Job {job_id} is not completed")
    
    if not job["result_file"] or not os.path.exists(job["result_file"]):
        raise HTTPException(status_code=404, detail=f"Result file for job {job_id} not found")
    
    return FileResponse(path=job["result_file"], filename=os.path.basename(job["result_file"]))


@app.get("/jobs/{job_id}/download-aggregated")
async def download_aggregated_result(job_id: str = Path(..., description="Job ID to download aggregated results for")):
    """
    Download the aggregated result file for a job.
    
    Args:
        job_id: Job ID to download aggregated results for
        
    Returns:
        File response with the aggregated result file
    """
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")
    
    job = jobs[job_id]
    
    if job["status"] != "Completed":
        raise HTTPException(status_code=400, detail=f"Job {job_id} is not completed")
    
    if not job.get("aggregated_result_file") or not os.path.exists(job["aggregated_result_file"]):
        raise HTTPException(status_code=404, detail=f"Aggregated result file for job {job_id} not found")
    
    return FileResponse(
        path=job["aggregated_result_file"], 
        filename=os.path.basename(job["aggregated_result_file"])
    )


@app.get("/jobs")
async def list_jobs():
    """
    List all jobs.
    
    Returns:
        List of all jobs
    """
    return list(jobs.values())
