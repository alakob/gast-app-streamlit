# Import Bakta environment setup early
import streamlit.bakta_env_setup
# Import Bakta environment setup early
import streamlit.bakta_env_setup
"""
AMR Prediction & Genome Annotation Streamlit App
"""
import streamlit as st
import threading
import time
import logging
from datetime import datetime
from pathlib import Path
import os
import tempfile
import json
import requests
# SSE implementation temporarily disabled
# from sseclient import SSEClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('gast-app')

# Import custom modules
import sys
import os

# Add the streamlit directory to the Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Add the parent directory to the path to make amr_predictor available if present

# Load the Bakta integration patch early to fix Docker/module issues
try:
    import bakta_integration_patch
    logger.info("‚úì Bakta integration patch loaded successfully")
except Exception as e:
    logger.error(f"Failed to load Bakta integration patch: {e}")
    
# Import the Bakta URL override module to ensure correct API endpoints
try:
    import bakta_url_override
    logger.info("‚úì Bakta API URL override loaded successfully")
except Exception as e:
    logger.warning(f"Failed to load Bakta URL override: {e}")
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)

# Now import the modules
import config
from api_client import create_amr_client, create_bakta_interface, BAKTA_AVAILABLE
# SSE implementation temporarily disabled
# from sse_client import get_sse_listener
from ui_components import (
    create_sidebar,
    create_annotation_settings_tab,
    create_sequence_input_tab,
    create_results_tab,
    create_job_management_tab,
    add_job_to_history
)
# Import integration UI components with fallback for Docker environment
try:
    from integration_ui import display_integrated_analysis_ui
    INTEGRATION_UI_AVAILABLE = True
except ImportError:
    # Create a fallback implementation when module is not available
    def display_integrated_analysis_ui():
        st.header("Integrated Analysis")
        st.info("Integrated analysis module is not available in this environment. Please update your Docker image or ensure the module is installed.")
        st.markdown("""
        This module connects Bakta annotation data with AMR prediction results to provide:
        - Correlation between genomic features and AMR genes
        - Integrated genome map visualization
        - Feature-AMR correlation analysis
        - Detailed feature analysis with AMR implications
        """)
    INTEGRATION_UI_AVAILABLE = False
    logging.warning("integration_ui module not found - using fallback implementation")
from utils import (
    is_valid_dna_sequence,
    get_sequence_statistics,
    parse_fasta_file,
    create_unique_job_name
)

# Display a warning if Bakta is not available
BAKTA_WARNING_DISPLAYED = False

# Set page config
logger.info("Initializing GAST application")
st.set_page_config(
    page_title=config.APP_TITLE,
    page_icon="üß¨",
    layout="wide",
    initial_sidebar_state="expanded"
)
logger.info("Page config set")

# Load custom CSS
def load_css(css_file):
    with open(css_file, 'r') as f:
        css = f'<style>{f.read()}</style>'
        st.markdown(css, unsafe_allow_html=True)

# Get the path to the custom CSS file
import os
current_dir = os.path.dirname(os.path.abspath(__file__))
css_path = os.path.join(current_dir, "custom.css")

# Load the custom CSS
if os.path.exists(css_path):
    load_css(css_path)

# Initialize session state
if "initialized" not in st.session_state:
    logger.info("First run - initializing session state")
    st.session_state.initialized = True
    st.session_state.sequence = ""
    st.session_state.sequence_valid = False
    st.session_state.submit_clicked = False
    st.session_state.amr_params = {}
    st.session_state.seq_params = {}
    st.session_state.enable_bakta = True  # Default to enabled
    st.session_state.using_real_bakta_api = True  # Force real API mode
    st.session_state.jobs = []
    st.session_state.active_tab = 0  # Track active tab index (0-based)
    
    # Initialize bakta_params with defaults from config
    st.session_state.bakta_params = {
        "genus": config.DEFAULT_GENUS,
        "species": config.DEFAULT_SPECIES,
        "strain": "",
        "complete_genome": False,
        "translation_table": config.DEFAULT_TRANSLATION_TABLE,
        "locus": "",
        "locus_tag": ""
    }
    logger.info("Session state initialized with default values")
else:
    logger.info("Session already initialized, reusing existing state")


def check_api_connectivity():
    """Check connectivity to the AMR and Bakta APIs."""
    logger.info("Checking API connectivity")
    
    # Check AMR API
    logger.info("Checking AMR API connectivity")
    try:
        # ALWAYS FORCE REAL API MODE - Disable mock data completely
        st.session_state["_mock_job_ids"] = set()  # Clear any mock job tracking
        st.session_state["_amr_mock_job_ids"] = set()  # Clear AMR-specific mock tracking
        st.session_state["using_real_amr_api"] = True  # Force real API mode
        st.session_state["mock_override"] = False  # Disable mock override
        logger.info("FORCING REAL AMR API MODE - Mock data disabled completely")
        
        client = create_amr_client()
        # Always use real API mode - no conditional check
        logger.info("AMR API forced to real mode - mock data disabled")
        st.session_state.update_amr_status("Connected (REAL MODE ONLY)", "success")
    except Exception as e:
        logger.error(f"AMR API connection failed: {str(e)}")
        st.session_state.update_amr_status(f"Not connected: {str(e)}", "error")
    
    # Check Bakta API if enabled
    if st.session_state.get("enable_bakta", False):
        logger.info("Bakta API enabled, checking connectivity")
        try:
            bakta_client = create_bakta_interface()
            # Check if we're in mock mode based on session state
            if st.session_state.get("using_real_bakta_api", False):
                logger.info("Bakta API connected and using real API")
                st.session_state.update_bakta_status("Connected", "success")
            else:
                logger.info("Bakta API in mock mode - NOT affecting AMR API mode")
                # Explicitly isolate Bakta mock mode from AMR API
                logger.info("Ensuring Bakta mock mode doesn't affect AMR API mode")
                st.session_state.update_bakta_status("Mock Mode", "warning")
        except Exception as e:
            logger.error(f"Bakta API connection failed: {str(e)}")
            st.session_state.update_bakta_status(f"Not connected: {str(e)}", "error")
            # Explicitly prevent Bakta errors from affecting AMR API mode
            logger.info("Preventing Bakta errors from affecting AMR API mode")
    else:
        logger.info("Bakta API disabled, skipping connectivity check")
        st.session_state.update_bakta_status("Disabled", "info")
        
    logger.info("API connectivity check completed")

def submit_amr_job(sequence):
    """
    Submit a sequence for AMR prediction.
    
    Args:
        sequence: DNA sequence string
    """
    logger.info("Preparing to submit AMR prediction job")
    try:
        # Create AMR API client
        logger.info("Creating AMR API client")
        client = create_amr_client()
        
        # Prepare parameters from session state
        params = st.session_state.amr_params.copy()
        logger.info(f"AMR parameters prepared: {params}")
        
        # Force real API mode if possible
        if st.session_state.get("using_real_amr_api", False):
            logger.info("Ensuring real API mode is active")
            st.session_state["mock_override"] = False
        
        # Submit job
        logger.info(f"Submitting sequence of length {len(sequence)} to AMR API")
        response = client.predict_amr(sequence, params)
        logger.info(f"AMR job submission response received: {response}")
        
        # Store job info in session state
        st.session_state.amr_job_id = response.get("job_id")
        st.session_state.amr_status = response.get("status", "PENDING")
        logger.info(f"AMR job ID: {st.session_state.amr_job_id}, initial status: {st.session_state.amr_status}")
        
        # Add to job history
        job_data = {
            "job_id": st.session_state.amr_job_id,
            "type": "AMR Prediction",
            "status": st.session_state.amr_status,
            "submitted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "params": params
        }
        logger.info(f"Adding AMR job to history: {job_data['job_id']}")
        add_job_to_history(job_data)
        
        # Force immediate switch to results tab
        logger.info("Switching to results tab immediately after job submission")
        st.session_state.active_tab = 2  # Results tab
        
        # Request immediate status check
        st.session_state.force_status_check = True
        
        # SSE implementation temporarily disabled
        # job_id = st.session_state.amr_job_id
        # if job_id:
        #     logger.info(f"Starting SSE listener for AMR job: {job_id}")
        #     sse_listener = get_sse_listener(config.AMR_API_URL)
        #     sse_listener.start_listening(job_id)
        #     logger.info("SSE listener started successfully")
        
        return st.session_state.amr_job_id
    
    except Exception as e:
        logger.error(f"Error submitting AMR job: {str(e)}", exc_info=True)
        st.session_state.amr_error = str(e)
        return None

def submit_bakta_job(sequence):
    """
    Submit a sequence for Bakta annotation.
    
    Args:
        sequence: DNA sequence string
    """
    logger.info("Preparing to submit Bakta annotation job")
    
    if not st.session_state.get("enable_bakta", False):
        logger.info("Bakta annotation is disabled, skipping submission")
        return None
    
    try:
        # Create a temporary file for the sequence
        logger.info("Creating temporary FASTA file for sequence")
        with tempfile.NamedTemporaryFile(suffix=".fasta", delete=False) as temp:
            # If sequence is in FASTA format, write as is, otherwise add a header
            if sequence.strip().startswith(">"):
                logger.info("Sequence is in FASTA format, using as-is")
                temp.write(sequence.encode())
            else:
                logger.info("Adding FASTA header to sequence")
                temp.write(f">streamlit_submission\n{sequence}".encode())
            
            temp_path = temp.name
            logger.info(f"Temporary FASTA file created at: {temp_path}")
        
        # Prepare configuration
        bakta_config = st.session_state.bakta_params.copy()
        logger.info(f"Bakta parameters prepared: {bakta_config}")
        
        # Generate a unique job name
        job_name = create_unique_job_name()
        logger.info(f"Generated unique job name: {job_name}")
        
        # First try to use our unified adapter if available
        use_unified_adapter = False
        try:
            from amr_predictor.bakta.unified_adapter import get_adapter, run_async
            logger.info("Found unified adapter module, attempting to use it")
            use_unified_adapter = True
        except ImportError as e:
            logger.warning(f"Unified adapter not available: {str(e)} - falling back to standard interface")
        
        # Submit job
        if use_unified_adapter:
            try:
                logger.info("Using unified Bakta adapter")
                adapter = get_adapter(environment="prod")
                # We need to run the async function with the sequence content, not the file path
                with open(temp_path, 'r') as f:
                    sequence_content = f.read()
                
                # Add job name to config
                bakta_config["name"] = job_name
                
                # Run the async submission function
                logger.info("Submitting job via unified adapter")
                response = run_async(adapter.submit_job, sequence_content, bakta_config)
                
                # Extract job_id from response
                job_id = response.get("id")
                st.session_state.bakta_job_secret = response.get("secret")
                
                logger.info(f"Job submission via unified adapter successful: {job_id}")
                
                # Store adapter in session state for future use
                try:
                    st.session_state.bakta_adapter = adapter
                    logger.info("Saved unified adapter to session state for reuse")
                except Exception as save_error:
                    logger.warning(f"Failed to save adapter to session state: {str(save_error)}")
            except Exception as adapter_error:
                logger.warning(f"Error using unified adapter: {str(adapter_error)} - falling back to standard interface")
                use_unified_adapter = False
        
        if not use_unified_adapter:
            # Create standard Bakta interface
            logger.info("Creating standard Bakta interface")
            bakta_interface = create_bakta_interface()
            
            logger.info("Submitting job to Bakta API via standard interface")
            job_id = bakta_interface.submit_job(
                fasta_data=temp_path,
                job_name=job_name,
                config_params=bakta_config
            )
            logger.info(f"Bakta job submitted successfully, job ID: {job_id}")
        
        # Clean up the temporary file
        if os.path.exists(temp_path):
            logger.info(f"Removing temporary FASTA file: {temp_path}")
            os.unlink(temp_path)
        
        # Store job info in session state
        st.session_state.bakta_job_id = job_id
        st.session_state.bakta_status = "PENDING"
        logger.info(f"Bakta job ID {job_id} stored in session state with status: PENDING")
        
        # Add to job history
        job_data = {
            "job_id": job_id,
            "type": "Bakta Annotation",
            "status": "PENDING",
            "submitted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "params": bakta_config
        }
        logger.info(f"Adding Bakta job to history: {job_id}")
        add_job_to_history(job_data)
        
        # SSE implementation temporarily disabled
        # if job_id:
        #     logger.info(f"Starting SSE listener for Bakta job: {job_id}")
        #     sse_listener = get_sse_listener(config.AMR_API_URL)
        #     sse_listener.start_listening(job_id)
        #     logger.info("SSE listener started successfully")
        
        return job_id
    
    except Exception as e:
        logger.error(f"Error submitting Bakta job: {str(e)}", exc_info=True)
        st.session_state.bakta_error = str(e)
        return None
    finally:
        # Ensure temp file is removed
        if 'temp_path' in locals() and os.path.exists(temp_path):
            logger.info(f"Cleaning up temporary file in finally block: {temp_path}")
            os.unlink(temp_path)

def check_job_status():
    """Check the status of submitted jobs."""
    logger.info("Checking status of submitted jobs")
    
    # Reset API connection if needed
    if "reset_api_connection" in st.session_state and st.session_state.reset_api_connection:
        logger.info("Resetting API connection to force fresh database check")
        client = create_amr_client()
        # If client was created successfully, reset the API connection flag
        st.session_state.reset_api_connection = False
        # Force real API mode if possible
        if st.session_state.get("using_real_amr_api", False):
            logger.info("Real API connection confirmed, forcing real mode")
            # Clear any mock job tracking to ensure we use real data
            if "_mock_job_ids" in st.session_state:
                logger.info("Clearing mock job tracking to ensure real database checks")
                st.session_state["_mock_job_ids"] = set()
    
    # Force real API mode if directly requested
    if "force_status_check" in st.session_state and st.session_state.force_status_check:
        logger.info("Force status check requested, ensuring real API mode")
        client = create_amr_client()
        st.session_state.force_status_check = False
        # Force into real mode if the API is available
        if st.session_state.get("using_real_amr_api", False):
            logger.info("Real API confirmed available, forcing real mode")
            st.session_state["mock_override"] = False
            # Clear any mock job tracking
            if "_mock_job_ids" in st.session_state:
                st.session_state["_mock_job_ids"] = set()
    
    # Always prioritize using real API if it's available
    using_real_api = st.session_state.get("using_real_amr_api", False)
    if using_real_api:
        logger.info("Using real API to check status")
        st.session_state["mock_override"] = False
    else:
        logger.warning("Using mock mode for status check - this may show incorrect data")
    
    # Check AMR prediction jobs
    status_changed = False
    if "amr_job_id" in st.session_state:
        amr_job_id = st.session_state.amr_job_id
        logger.info(f"Checking AMR job status for ID: {amr_job_id}")
        status_changed = _legacy_check_amr_status(amr_job_id) or status_changed
    
    # Check Bakta annotation jobs
    if "bakta_job_id" in st.session_state:
        bakta_job_id = st.session_state.bakta_job_id
        logger.info(f"Checking Bakta job status for ID: {bakta_job_id}")
        status_changed = _legacy_check_bakta_status(bakta_job_id) or status_changed
        
    # If status changed to completed, make sure we're on the results tab
    if status_changed:
        logger.info("Job status changed, checking if we need to switch tabs")
        if st.session_state.get("active_tab", 0) != 2:
            logger.info("Switching to results tab due to status change")
            st.session_state.active_tab = 2
            # Rerun needed to refresh UI
            st.rerun()
    
# Legacy status checking functions as fallback
def _legacy_check_amr_status(amr_job_id):
    """Method to check AMR job status via database or API."""
    logger.info(f"Checking AMR job status for: {amr_job_id}")
    status_changed = False
    
    try:
        # Force real API mode if available
        using_real_api = st.session_state.get("using_real_amr_api", False)
        if using_real_api:
            st.session_state["mock_override"] = False
            # Explicitly clear this job from mock tracking if it was mistakenly added
            mock_jobs = st.session_state.get("_mock_job_ids", set())
            if amr_job_id in mock_jobs:
                logger.info(f"Removing job {amr_job_id} from mock tracking to force real DB check")
                mock_jobs.remove(amr_job_id)
                st.session_state["_mock_job_ids"] = mock_jobs
        
        # Create a fresh API client for status check
        logger.info("Creating AMR API client for status check")
        client = create_amr_client()
        
        # Log what mode we're using
        if st.session_state.get("using_real_amr_api", False):
            logger.info("Using real API to check job status")
        else:
            logger.warning("Using mock mode for status check - may show incorrect data")
        
        # Request status from API/database
        logger.info(f"Requesting status for AMR job: {amr_job_id}")
        status_response = client.get_prediction_status(amr_job_id)
        
        # Extract status from response and normalize to uppercase
        raw_status = status_response.get("status", "UNKNOWN")
        status = raw_status.upper() if isinstance(raw_status, str) else raw_status
        logger.info(f"AMR job status received: {status} (original: {raw_status})")
        
        # Check if we got data from PostgreSQL
        db_source = status_response.get("source", "unknown")
        logger.info(f"Status data source: {db_source}")
        
        # If we got data from the database, ensure we're using real API mode
        if db_source == "database" and not using_real_api:
            logger.info("Received database data but not in real API mode - fixing this")
            st.session_state["using_real_amr_api"] = True
            using_real_api = True
        
        # Force refresh of session state status to ensure consistency
        previous_status = st.session_state.get("amr_status", "UNKNOWN")
        if previous_status != status:
            logger.info(f"AMR job status changed: {previous_status} -> {status}")
            status_changed = True
            
            # If the status indicates completion but the UI shows running,
            # we need to force a refresh of results
            if status in ["SUCCESSFUL", "COMPLETE", "COMPLETED"] and previous_status not in ["SUCCESSFUL", "COMPLETE", "COMPLETED"]:
                logger.info("Job completed - clearing results cache to force refresh")
                if "amr_results" in st.session_state:
                    del st.session_state.amr_results
                # Force a page refresh to ensure we show the latest data
                logger.info("Job completed - requesting UI refresh")
                st.session_state.force_refresh = True
        
        # Set current status in session state
        st.session_state.amr_status = status
        
        # Update job in history - ensure status is in sync
        logger.info("Updating AMR job status in job history")
        updated = False
        for job in st.session_state.jobs:
            if job.get("job_id") == amr_job_id:
                old_status = job.get("status")
                job["status"] = status
                if old_status != status:
                    logger.info(f"Updated job history status from {old_status} to {status}")
                    status_changed = True
                updated = True
                break
        
        if not updated and status not in ["UNKNOWN", "ERROR"]:
            logger.warning(f"Job {amr_job_id} not found in job history, adding it now")
            st.session_state.jobs.append({
                "job_id": amr_job_id,
                "type": "AMR Prediction",
                "status": status,
                "submitted_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            status_changed = True
        
        # If job is complete, get results from PostgreSQL/disk
        if status in ["SUCCESSFUL", "COMPLETE", "COMPLETED"]:
            logger.info(f"AMR job {amr_job_id} completed successfully, fetching results")
            # Clear any cached results to force fresh retrieval from database
            if "amr_results" in st.session_state:
                logger.info("Clearing cached AMR results to get fresh data")
                del st.session_state.amr_results
                
            logger.info("Requesting AMR prediction results from database/disk")
            results = client.get_prediction_results(amr_job_id)
            
            if results:
                logger.info(f"AMR results received: {results.get('status', 'NO_STATUS')}")
                st.session_state.amr_results = results
                
                # Force switch to results tab if not already there
                if st.session_state.get("active_tab", 0) != 2:
                    logger.info("Switching to results tab since job is complete")
                    st.session_state.active_tab = 2
                    status_changed = True
            else:
                logger.warning("No results data received from API")
        elif status == "FAILED":
            logger.error(f"AMR job {amr_job_id} failed")
            st.session_state.amr_error = "Job processing failed on the server"
        elif status == "CANCELLED":
            logger.warning(f"AMR job {amr_job_id} was cancelled")
    except Exception as e:
        logger.error(f"Error checking AMR job status: {str(e)}", exc_info=True)
        st.session_state.amr_error = str(e)
        
    # Return whether the status changed to inform caller
    return status_changed

def _legacy_check_bakta_status(bakta_job_id):
    """Legacy method to check Bakta job status via polling (fallback)."""
    logger.info(f"Using legacy polling to check Bakta job status for: {bakta_job_id}")
    try:
        logger.info("Creating Bakta API interface for status check")
        bakta_interface = create_bakta_interface()
        logger.info(f"Requesting status for Bakta job: {bakta_job_id}")
        status = bakta_interface.get_job_status(bakta_job_id)
        
        previous_status = st.session_state.get("bakta_status", "UNKNOWN")
        if status != previous_status:
            logger.info(f"Bakta job status changed: {previous_status} -> {status}")
            
        st.session_state.bakta_status = status
        logger.info(f"Bakta job status updated: {status}")
        
        # Update job in history
        logger.info("Updating Bakta job status in job history")
        for job in st.session_state.jobs:
            if job.get("job_id") == bakta_job_id:
                job["status"] = status
        
        # If job is complete, get results
        if status == "SUCCESSFUL":
            logger.info(f"Bakta job {bakta_job_id} completed successfully, fetching results")
            if "bakta_results" not in st.session_state:
                logger.info("Requesting Bakta annotation results")
                results = bakta_interface.get_job_results(bakta_job_id)
                logger.info("Bakta results received and stored in session state")
                st.session_state.bakta_results = results
        elif status == "FAILED":
            logger.error(f"Bakta job {bakta_job_id} failed")
        elif status == "CANCELLED":
            logger.warning(f"Bakta job {bakta_job_id} was cancelled")
    except Exception as e:
        logger.error(f"Error checking Bakta job status: {str(e)}", exc_info=True)
        st.session_state.bakta_error = str(e)
    else:
        logger.info("No Bakta job ID in session state, skipping Bakta status check")
        
    logger.info("Job status check completed")

def process_submission():
    """Process submission of sequence for analysis."""
    logger.info("Processing sequence submission")
    if not st.session_state.sequence_valid:
        logger.warning("Invalid sequence detected, aborting submission")
        st.error("Please provide a valid DNA sequence")
        return
    
    logger.info(f"Submitting valid sequence of length {len(st.session_state.sequence)}")
    with st.spinner("Submitting sequence for analysis..."):
        # Clear any previous results
        if "amr_results" in st.session_state:
            logger.info("Clearing previous AMR results")
            del st.session_state.amr_results
        if "bakta_results" in st.session_state:
            logger.info("Clearing previous Bakta results")
            del st.session_state.bakta_results
        
        # Submit AMR job
        logger.info("Initiating AMR job submission")
        amr_job_id = submit_amr_job(st.session_state.sequence)
        
        if amr_job_id:
            logger.info(f"AMR job submission successful, job ID: {amr_job_id}")
            st.session_state.amr_job_id = amr_job_id
            st.session_state.amr_status = "PENDING"
            
            # Add AMR job to history
            amr_job_data = {
                "job_id": amr_job_id,
                "type": "AMR Prediction",
                "status": "PENDING",
                "submitted": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            add_job_to_history(amr_job_data)
            
            st.success(f"AMR prediction job submitted (ID: {amr_job_id})")
        else:
            logger.error("AMR job submission failed, no job ID returned")
        
        # Submit Bakta job if enabled
        if st.session_state.get("enable_bakta", False):
            logger.info("Bakta enabled, initiating Bakta job submission")
            bakta_job_id = submit_bakta_job(st.session_state.sequence)
            
            if bakta_job_id:
                logger.info(f"Bakta job submission successful, job ID: {bakta_job_id}")
                # Store job ID and status in session state
                st.session_state.bakta_job_id = bakta_job_id
                st.session_state.bakta_status = "PENDING"
                
                # Make sure the job is added to history
                bakta_job_data = {
                    "job_id": bakta_job_id,
                    "type": "Bakta Annotation",
                    "status": "PENDING",
                    "submitted": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                add_job_to_history(bakta_job_data)
                
                st.success(f"Bakta annotation job submitted (ID: {bakta_job_id})")
            else:
                logger.error("Bakta job submission failed, no job ID returned")
        else:
            logger.info("Bakta disabled, skipping Bakta job submission")
            # Remove bakta job data from session state if it exists
            if "bakta_job_id" in st.session_state:
                logger.info("Removing bakta_job_id from session state as Bakta is disabled")
                del st.session_state.bakta_job_id
            if "bakta_status" in st.session_state:
                logger.info("Removing bakta_status from session state as Bakta is disabled")
                del st.session_state.bakta_status
            if "bakta_results" in st.session_state:
                logger.info("Removing bakta_results from session state as Bakta is disabled")
                del st.session_state.bakta_results
    
    logger.info("Sequence submission processing completed")
    # Force real API mode
    logger.info("Forcing real API mode to ensure database checks")
    st.session_state["using_real_amr_api"] = True
    if "_mock_job_ids" in st.session_state:
        logger.info("Clearing mock job tracking")
        st.session_state["_mock_job_ids"] = set()
    
    # Auto-navigate to results tab with force refresh
    logger.info("Auto-navigating to results tab")
    st.session_state.active_tab = 2  # Results tab
    
    # Set flag to force an immediate rerun to apply tab change
    st.session_state.force_rerun = True

# Create the sidebar
create_sidebar()

# Check API connectivity on app startup
check_api_connectivity()

# Add title and subtitle with custom styling
st.markdown("""
<div style='text-align: center;'>
    <h1 class='dna-title'>
        <span style='font-size: 1.5em;'>üß¨</span> 
        <span style='color: #1cb3e0;'>g</span>enomic
        <span style='color: #1A5276;'>A</span>ntimicrobial
        <span style='color: #1cb3e0;'>S</span>usceptibility
        <span style='color: #1A5276;'>T</span>esting
    </h1>
    <p style='font-size: 1.2em; color: #d2d2d6;'>Advanced machine learning platform for predicting antimicrobial resistance from bacterial genome sequences</p>
    <div class='blue-gradient-hr'></div>
</div>
""", unsafe_allow_html=True)

# Create the main tab structure
logger.info("Setting up main tab structure")
tabs = ["Annotation Settings", "Sequence Input", "Results", "Job Management"]

# Only add the Integrated Analysis tab if the module is available
if INTEGRATION_UI_AVAILABLE:
    tabs.append("Integrated Analysis")
    logger.info("Integrated Analysis tab added to UI")
else:
    logger.warning("Integrated Analysis tab not available - module not found")

# Get active tab from session state (default to 0 if not set)
active_tab_index = st.session_state.get("active_tab", 0)
logger.info(f"Active tab index from session state: {active_tab_index}")

# Create tabs and store them in a list for easier access by index
tab_list = st.tabs(tabs)
logger.info(f"Created {len(tab_list)} tabs: {tabs}")

# For backwards compatibility with existing code
if len(tab_list) >= 5:
    tab1, tab2, tab3, tab4, tab5 = tab_list
else:
    tab1, tab2, tab3, tab4 = tab_list
    # Create a dummy tab5 that won't be used
    tab5 = None

# This is a workaround for Streamlit's lack of a direct API to programmatically
# select a tab. We use the tab container's existing CSS classes to select the active tab.
if active_tab_index > 0:  # Only inject custom JS if not on the first tab
    js = f"""
    <script>
        // Wait for DOM to be ready
        document.addEventListener('DOMContentLoaded', (event) => {{  
            // Small delay to ensure Streamlit components are loaded
            setTimeout(() => {{                                      
                const tabs = window.parent.document.querySelectorAll('button[data-baseweb="tab"]');
                if (tabs.length >= {active_tab_index + 1}) {{
                    tabs[{active_tab_index}].click();
                }}
            }}, 100);
        }});
    </script>
    """
    st.components.v1.html(js, height=0)

# Populate the tabs
with tab1:
    create_annotation_settings_tab()
    
    # Display a warning if Bakta is not available
    if not BAKTA_AVAILABLE and not BAKTA_WARNING_DISPLAYED:
        st.warning(
            "‚ö†Ô∏è The Bakta module is not available. This app is running in demo mode with mock implementations. "
            "To use the full functionality, make sure the amr_predictor package is installed and in the Python path."
        )
        globals()['BAKTA_WARNING_DISPLAYED'] = True

with tab2:
    create_sequence_input_tab()
    
    # Process submission if button clicked
    if st.session_state.submit_clicked:
        process_submission()
        # Reset button state
        st.session_state.submit_clicked = False
        # Switch to Results tab (index 2)
        st.session_state.active_tab = 2
        # Force immediate check of job status with real API
        st.session_state.force_status_check = True
        st.rerun()  # Rerun the app to activate the selected tab

with tab3:
    create_results_tab()
    
    # Check if force_rerun is requested
    if "force_rerun" in st.session_state and st.session_state.force_rerun:
        logger.info("Force rerun detected in results tab - forcing immediate database check")
        # Force real API mode
        st.session_state["using_real_amr_api"] = True
        # Clear any mock job tracking
        if "_mock_job_ids" in st.session_state:
            st.session_state["_mock_job_ids"] = set()
        # Reset the flag
        st.session_state.force_rerun = False
        # Force a page rerun to apply changes
        st.rerun()
    
    # Check job statuses periodically
    if "amr_job_id" in st.session_state or "bakta_job_id" in st.session_state:
        # Always force a fresh status check in real mode
        st.session_state["using_real_amr_api"] = True
        check_job_status()
        
        # Add refresh button if jobs are still running
        status_list = ["SUCCESSFUL", "FAILED", "CANCELLED", "COMPLETED", "COMPLETE"]
        if (st.session_state.get("amr_status", "").upper() not in status_list or
            st.session_state.get("bakta_status", "").upper() not in status_list):
            
            if st.button("Refresh Status"):
                # Force database check with real mode
                st.session_state["using_real_amr_api"] = True
                st.session_state["_mock_job_ids"] = set()
                check_job_status()
                st.rerun()

with tab4:
    create_job_management_tab()

# Integrated Analysis tab (only render if the tab exists)
if tab5 is not None:
    with tab5:
        logger.info("Rendering Integrated Analysis tab")
        display_integrated_analysis_ui()

# Revert to traditional polling for running jobs
if "amr_job_id" in st.session_state or "bakta_job_id" in st.session_state:
    # Check if status is one that requires updates
    amr_status = st.session_state.get("amr_status", "")
    bakta_status = st.session_state.get("bakta_status", "")
    
    # Non-final statuses
    running_statuses = ["SUBMITTED", "PENDING", "QUEUED", "RUNNING", "PROCESSING"]
    
    # Complete status values (case-insensitive check)
    complete_statuses = ["COMPLETED", "COMPLETE", "SUCCESSFUL", "SUCCESS", "DONE", "FINISHED"]
    
    # First, normalize statuses for case-insensitive comparison
    amr_status_upper = amr_status.upper() if amr_status else ""
    bakta_status_upper = bakta_status.upper() if bakta_status else ""
    
    # Check if any job is actually in a running state
    is_running = (amr_status_upper in [s.upper() for s in running_statuses] or 
                 bakta_status_upper in [s.upper() for s in running_statuses])
    
    # Check if we have at least one job but none are completed
    has_jobs = ("amr_job_id" in st.session_state or "bakta_job_id" in st.session_state)
    is_completed = (amr_status_upper in [s.upper() for s in complete_statuses] or 
                   bakta_status_upper in [s.upper() for s in complete_statuses])
    
    # Only show running message and auto-refresh if jobs are actually running
    if is_running and has_jobs and not is_completed:
        st.markdown("---")
        st.info("Jobs are running. Status will refresh automatically.")
        
        # Check job status manually
        check_job_status()
        
        # Add a small delay and rerun to refresh the UI
        time.sleep(5)  # 5 second refresh interval
        st.rerun()
